{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4878/3916160878.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pprint import pprint\n",
    "from belt_nlp.splitting import transform_single_text\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_gui\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text_path, tokenizer, embedder, get_embeddings=False):\n",
    "    with open(text_path, 'r') as text_file:\n",
    "        text = text_file.read()\n",
    "\n",
    "    labels_path = text_path.replace('.txt', '-labels-subtask-3.txt').replace('-articles-subtask-3', '-labels-subtask-3-spans')\n",
    "    with open(labels_path, 'r') as labels_file:\n",
    "        labels = labels_file.read()\n",
    "        labels = [l.split('\\t') for l in labels.split('\\n')][:-1]\n",
    "    \n",
    "    res = dict()\n",
    "\n",
    "    tokens = tokenizer(text, add_special_tokens=False, truncation=False, return_tensors='pt')\n",
    "    res['tokens'] = tokens\n",
    "    res['num_tokens'] = len(tokens['input_ids'][0])\n",
    "    input_ids, attention_mask = transform_single_text(\n",
    "        text,\n",
    "        tokenizer,\n",
    "        chunk_size=510,\n",
    "        stride=300,\n",
    "        minimal_chunk_length=20,\n",
    "        maximal_text_length=None\n",
    "    )\n",
    "    res['chunks'] = input_ids\n",
    "    res['attn_mask'] = attention_mask\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    embedder.to(device)\n",
    "    embeddings = []\n",
    "    for i in range(len(res['chunks'])):\n",
    "        with torch.no_grad():\n",
    "            output = embedder(res['chunks'][i].unsqueeze(0).to(device), res['attn_mask'][i].unsqueeze(0).to(device))\n",
    "        _start = 0 if i == 0 else 210\n",
    "        _end = list(res['attn_mask'][i]).index(0) if 0 in res['attn_mask'][i] else 0\n",
    "        last_hidden_state = output.last_hidden_state[:, 1+_start:-1+_end, :].detach().cpu()\n",
    "        embeddings.append(last_hidden_state)\n",
    "    embeddings = torch.cat(embeddings, dim=1)\n",
    "\n",
    "    # SPANS\n",
    "    spans = [text[int(l[2]):int(l[3])] for l in labels]\n",
    "    res['spans'] = spans\n",
    "\n",
    "    # NONE CLASS\n",
    "    space_ids = [i for i in range(len(text)) if ' ' == text[i]]\n",
    "    # space_ids = [i for i in range(len(text)) if re.fullmatch(r'\\s', text[i])]\n",
    "    span_edges = [(int(l[2]), int(l[3])) for l in labels]\n",
    "    def overlap(s, e, edgs):\n",
    "        return any([not (e < edgs[i][0] or s > edgs[i][1]) for i in range(len(edgs))])\n",
    "    \n",
    "    none_classes, none_spans = [], []\n",
    "    for i in range(len(spans)):\n",
    "        _len = len(spans[i].split())\n",
    "        start = space_ids.index(np.random.choice(space_ids))\n",
    "        end = start + _len\n",
    "        counter = 1\n",
    "        while overlap(start, end, span_edges) and counter < 200:\n",
    "            start = space_ids.index(np.random.choice(space_ids))\n",
    "            end = start + _len\n",
    "            counter += 1\n",
    "        none_spans.append(' '.join(text.split()[start:end + 1]))\n",
    "        none_classes.append('NoClass')\n",
    "\n",
    "    res['spans'].extend(none_spans)\n",
    "\n",
    "    # SPAN EMDEDDINGS\n",
    "    span_tokens = [\n",
    "        tokenizer(span, add_special_tokens=False, truncation=False)\n",
    "        for span in res['spans']\n",
    "    ]\n",
    "    span_embeddings = []\n",
    "    res['classes'] = []\n",
    "    for j in range(len(span_tokens)):\n",
    "        try:\n",
    "            for i in range(len(tokens['input_ids'][0])):\n",
    "                if (tokens['input_ids'][0][i:i + len(span_tokens[j]['input_ids'])].numpy() == span_tokens[j]['input_ids']).all():\n",
    "                    span_embeddings.append(embeddings[:, i:i + len(span_tokens[j]['input_ids']), :])\n",
    "                    try:\n",
    "                        res['classes'].append(labels[j][1])\n",
    "                    except IndexError as e:\n",
    "                        pass\n",
    "                    break\n",
    "        except ValueError as e:\n",
    "            print(text_path, labels_path)\n",
    "    res['span_embeddings'] = span_embeddings\n",
    "\n",
    "    res['classes'].extend(none_classes)\n",
    "\n",
    "    text = text.lower()\n",
    "    for l in labels:\n",
    "        text = text.replace(text[int(l[2]):int(l[3])], text[int(l[2]):int(l[3])].upper())\n",
    "    res['text'] = text\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Democrats, the test results are back, and Donald Trump is your daddy',\n",
      " 'along with three laughing emojis',\n",
      " \"mocked Democrats saying that the 'test results' show that President Donald \"\n",
      " \"Trump is their 'daddy'\",\n",
      " 'trumpeted',\n",
      " 'feverish predictions',\n",
      " \"over the conclusion of special counsel Robert Mueller's investigation. \"\n",
      " \"'Democrats, the test results\",\n",
      " \"Trump is your daddy,' read the\",\n",
      " \"the report does not find that they committed crimes. Trump's business, his \"\n",
      " 'charity and his presidential',\n",
      " 'and his',\n",
      " 'trumpeted reports that']\n",
      "['Loaded_Language',\n",
      " 'Loaded_Language',\n",
      " 'Loaded_Language',\n",
      " 'Loaded_Language',\n",
      " 'Loaded_Language',\n",
      " 'NoClass',\n",
      " 'NoClass',\n",
      " 'NoClass',\n",
      " 'NoClass',\n",
      " 'NoClass']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "embedder = BertModel.from_pretrained('SpanBERT/spanbert-base-cased', add_pooling_layer=False)\n",
    "\n",
    "path = np.random.choice(glob.glob('./dataset/data/en/dev-articles-subtask-3/*.txt'))\n",
    "r = process_text(path, tokenizer, embedder)\n",
    "pprint(r['spans'])\n",
    "pprint(r['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 159/446 [00:52<00:44,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article787730392.txt ./dataset/data/en/train-labels-subtask-3-spans/article787730392-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 221/446 [01:10<00:50,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article755459860.txt ./dataset/data/en/train-labels-subtask-3-spans/article755459860-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 231/446 [01:32<08:54,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article764664283.txt ./dataset/data/en/train-labels-subtask-3-spans/article764664283-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 294/446 [01:52<01:09,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article706600938.txt ./dataset/data/en/train-labels-subtask-3-spans/article706600938-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 334/446 [02:10<00:36,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article739091070.txt ./dataset/data/en/train-labels-subtask-3-spans/article739091070-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 346/446 [02:16<00:27,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article757713354.txt ./dataset/data/en/train-labels-subtask-3-spans/article757713354-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 397/446 [02:34<00:06,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/train-articles-subtask-3/article999000870.txt ./dataset/data/en/train-labels-subtask-3-spans/article999000870-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [02:50<00:00,  2.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./train_embeddings/article727658675_0.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>will proclaim, with all clarity, unambiguity a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./train_embeddings/article727658675_1.pt</td>\n",
       "      <td>Appeal_to_Fear-Prejudice</td>\n",
       "      <td>and it would be a beautiful common voice defen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./train_embeddings/article727658675_2.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>would be a beautiful common voice defending th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./train_embeddings/article727658675_3.pt</td>\n",
       "      <td>Exaggeration-Minimisation</td>\n",
       "      <td>plague</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./train_embeddings/article727658675_4.pt</td>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>Some of these norms, Schneider told LifeSiteNe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./train_embeddings/article727658675_5.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>Such a reading is causing “rampant confusion,”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./train_embeddings/article727658675_6.pt</td>\n",
       "      <td>Exaggeration-Minimisation</td>\n",
       "      <td>plague of divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./train_embeddings/article727658675_7.pt</td>\n",
       "      <td>Obfuscation-Vagueness-Confusion</td>\n",
       "      <td>This is contrary to Divine Revelation,” Schnei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./train_embeddings/article727658675_8.pt</td>\n",
       "      <td>False_Dilemma-No_Choice</td>\n",
       "      <td>the “beautiful explanations” that are being pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./train_embeddings/article727658675_9.pt</td>\n",
       "      <td>Causal_Oversimplification</td>\n",
       "      <td>As Successors of the Apostles, Schneider said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./train_embeddings/article727658675_10.pt</td>\n",
       "      <td>False_Dilemma-No_Choice</td>\n",
       "      <td>As Successors of the Apostles, Schneider said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./train_embeddings/article727658675_11.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>“could not act in another way.” Bishop Marian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./train_embeddings/article727658675_12.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>of signatories to nine. To date, in addition t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./train_embeddings/article727658675_13.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>behalf of the Swiss Bishops’ Conference, and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./train_embeddings/article727658675_14.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>Schneider told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./train_embeddings/article727658675_15.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>truths of the Church, and it would be a beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./train_embeddings/article727658675_16.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>of Pope Francis’ apostolic exhortation on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./train_embeddings/article727658675_17.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>not act in another</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./train_embeddings/article727658675_18.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>The profession’s most recent signatory, Bishop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./train_embeddings/article727658675_19.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>to the “Profession of Immutable Truths about S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path  \\\n",
       "0    ./train_embeddings/article727658675_0.pt   \n",
       "1    ./train_embeddings/article727658675_1.pt   \n",
       "2    ./train_embeddings/article727658675_2.pt   \n",
       "3    ./train_embeddings/article727658675_3.pt   \n",
       "4    ./train_embeddings/article727658675_4.pt   \n",
       "5    ./train_embeddings/article727658675_5.pt   \n",
       "6    ./train_embeddings/article727658675_6.pt   \n",
       "7    ./train_embeddings/article727658675_7.pt   \n",
       "8    ./train_embeddings/article727658675_8.pt   \n",
       "9    ./train_embeddings/article727658675_9.pt   \n",
       "10  ./train_embeddings/article727658675_10.pt   \n",
       "11  ./train_embeddings/article727658675_11.pt   \n",
       "12  ./train_embeddings/article727658675_12.pt   \n",
       "13  ./train_embeddings/article727658675_13.pt   \n",
       "14  ./train_embeddings/article727658675_14.pt   \n",
       "15  ./train_embeddings/article727658675_15.pt   \n",
       "16  ./train_embeddings/article727658675_16.pt   \n",
       "17  ./train_embeddings/article727658675_17.pt   \n",
       "18  ./train_embeddings/article727658675_18.pt   \n",
       "19  ./train_embeddings/article727658675_19.pt   \n",
       "\n",
       "                              class  \\\n",
       "0                   Loaded_Language   \n",
       "1          Appeal_to_Fear-Prejudice   \n",
       "2                   Loaded_Language   \n",
       "3         Exaggeration-Minimisation   \n",
       "4               Appeal_to_Authority   \n",
       "5                   Loaded_Language   \n",
       "6         Exaggeration-Minimisation   \n",
       "7   Obfuscation-Vagueness-Confusion   \n",
       "8           False_Dilemma-No_Choice   \n",
       "9         Causal_Oversimplification   \n",
       "10          False_Dilemma-No_Choice   \n",
       "11                          NoClass   \n",
       "12                          NoClass   \n",
       "13                          NoClass   \n",
       "14                          NoClass   \n",
       "15                          NoClass   \n",
       "16                          NoClass   \n",
       "17                          NoClass   \n",
       "18                          NoClass   \n",
       "19                          NoClass   \n",
       "\n",
       "                                                 span  \n",
       "0   will proclaim, with all clarity, unambiguity a...  \n",
       "1   and it would be a beautiful common voice defen...  \n",
       "2   would be a beautiful common voice defending th...  \n",
       "3                                              plague  \n",
       "4   Some of these norms, Schneider told LifeSiteNe...  \n",
       "5   Such a reading is causing “rampant confusion,”...  \n",
       "6                                   plague of divorce  \n",
       "7   This is contrary to Divine Revelation,” Schnei...  \n",
       "8   the “beautiful explanations” that are being pr...  \n",
       "9   As Successors of the Apostles, Schneider said ...  \n",
       "10  As Successors of the Apostles, Schneider said ...  \n",
       "11  “could not act in another way.” Bishop Marian ...  \n",
       "12  of signatories to nine. To date, in addition t...  \n",
       "13  behalf of the Swiss Bishops’ Conference, and a...  \n",
       "14                                     Schneider told  \n",
       "15  truths of the Church, and it would be a beauti...  \n",
       "16  of Pope Francis’ apostolic exhortation on the ...  \n",
       "17                                 not act in another  \n",
       "18  The profession’s most recent signatory, Bishop...  \n",
       "19  to the “Profession of Immutable Truths about S...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "embedder = BertModel.from_pretrained('SpanBERT/spanbert-base-cased', add_pooling_layer=False)\n",
    "\n",
    "paths, classes, spans = [], [], []\n",
    "\n",
    "for text_path in tqdm(glob.glob('./dataset/data/en/train-articles-subtask-3/*.txt')):\n",
    "    r = process_text(text_path, tokenizer, embedder)\n",
    "    for i in range(len(r['span_embeddings'])):\n",
    "        torch.save(r['span_embeddings'][i],\n",
    "                   './train_embeddings/' + text_path.split('/')[-1].replace('.txt', '') + f'_{i}' + '.pt')\n",
    "        paths.append('./train_embeddings/' + text_path.split('/')[-1].replace('.txt', '') + f'_{i}' + '.pt')\n",
    "        classes.append(r['classes'][i])\n",
    "        spans.append(r['spans'][i])\n",
    "\n",
    "df = pd.DataFrame({'path': paths, 'class': classes, 'span': spans})\n",
    "df.to_csv('train.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>./train_embeddings/article790266787_17.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>this other con artist, Avenatti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>./train_embeddings/article783702663_325.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>must be the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>./train_embeddings/article779394730_33.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>but no thanks. You can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9776</th>\n",
       "      <td>./train_embeddings/article697959084_4.pt</td>\n",
       "      <td>Repetition</td>\n",
       "      <td>it is strikingly obvious that authorities have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6886</th>\n",
       "      <td>./train_embeddings/article782448403_21.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>for 3-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7731</th>\n",
       "      <td>./train_embeddings/article764664283_47.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>naïve, inaccurate, misleadingly sunny view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>./train_embeddings/article765385479_62.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>the terrorist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>./train_embeddings/article762546428_30.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>homeschooling are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>./train_embeddings/article999001419_10.pt</td>\n",
       "      <td>Exaggeration-Minimisation</td>\n",
       "      <td>a great First Amendment victory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>./train_embeddings/article777869943_14.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>a horrifying scene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6653</th>\n",
       "      <td>./train_embeddings/article706501640_4.pt</td>\n",
       "      <td>Repetition</td>\n",
       "      <td>deadly bacteria could mutate and become untrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>./train_embeddings/article783702663_218.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>imperiously scoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>./train_embeddings/article724791253_2.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>hypocrisy and double standards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>./train_embeddings/article787730392_1.pt</td>\n",
       "      <td>Appeal_to_Hypocrisy</td>\n",
       "      <td>If These Were White Christians…No One Would Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>./train_embeddings/article790667730_15.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>been through hell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>./train_embeddings/article765385479_77.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>pioneering feminists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>./train_embeddings/article761955563_29.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>be trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>./train_embeddings/article765197039_22.pt</td>\n",
       "      <td>Repetition</td>\n",
       "      <td>entire religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>./train_embeddings/article999000136_26.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>undercut by a member of his own staff. Not tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>./train_embeddings/article787002327_88.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>West -- on which the political and cultural es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path                      class  \\\n",
       "2261   ./train_embeddings/article790266787_17.pt      Name_Calling-Labeling   \n",
       "7495  ./train_embeddings/article783702663_325.pt                    NoClass   \n",
       "6370   ./train_embeddings/article779394730_33.pt                    NoClass   \n",
       "9776    ./train_embeddings/article697959084_4.pt                 Repetition   \n",
       "6886   ./train_embeddings/article782448403_21.pt                    NoClass   \n",
       "7731   ./train_embeddings/article764664283_47.pt            Loaded_Language   \n",
       "5407   ./train_embeddings/article765385479_62.pt      Name_Calling-Labeling   \n",
       "7977   ./train_embeddings/article762546428_30.pt                    NoClass   \n",
       "8015   ./train_embeddings/article999001419_10.pt  Exaggeration-Minimisation   \n",
       "1280   ./train_embeddings/article777869943_14.pt            Loaded_Language   \n",
       "6653    ./train_embeddings/article706501640_4.pt                 Repetition   \n",
       "7388  ./train_embeddings/article783702663_218.pt            Loaded_Language   \n",
       "8443    ./train_embeddings/article724791253_2.pt            Loaded_Language   \n",
       "4969    ./train_embeddings/article787730392_1.pt        Appeal_to_Hypocrisy   \n",
       "8564   ./train_embeddings/article790667730_15.pt            Loaded_Language   \n",
       "5422   ./train_embeddings/article765385479_77.pt      Name_Calling-Labeling   \n",
       "3997   ./train_embeddings/article761955563_29.pt                    NoClass   \n",
       "2614   ./train_embeddings/article765197039_22.pt                 Repetition   \n",
       "1490   ./train_embeddings/article999000136_26.pt                    NoClass   \n",
       "3164   ./train_embeddings/article787002327_88.pt                    NoClass   \n",
       "\n",
       "                                                   span  \n",
       "2261                    this other con artist, Avenatti  \n",
       "7495                                        must be the  \n",
       "6370                             but no thanks. You can  \n",
       "9776  it is strikingly obvious that authorities have...  \n",
       "6886                                            for 3-D  \n",
       "7731         naïve, inaccurate, misleadingly sunny view  \n",
       "5407                                      the terrorist  \n",
       "7977                                  homeschooling are  \n",
       "8015                    a great First Amendment victory  \n",
       "1280                                 a horrifying scene  \n",
       "6653  deadly bacteria could mutate and become untrea...  \n",
       "7388                                 imperiously scoffs  \n",
       "8443                     hypocrisy and double standards  \n",
       "4969  If These Were White Christians…No One Would Ba...  \n",
       "8564                                  been through hell  \n",
       "5422                               pioneering feminists  \n",
       "3997                                          be trying  \n",
       "2614                                    entire religion  \n",
       "1490  undercut by a member of his own staff. Not tha...  \n",
       "3164  West -- on which the political and cultural es...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/90 [00:00<01:25,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article820791520.txt ./dataset/data/en/dev-labels-subtask-3-spans/article820791520-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 17/90 [00:04<00:12,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article815858385.txt ./dataset/data/en/dev-labels-subtask-3-spans/article815858385-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 22/90 [00:05<00:13,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article813949697.txt ./dataset/data/en/dev-labels-subtask-3-spans/article813949697-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 27/90 [00:07<00:14,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833039623.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833039623-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 28/90 [00:08<00:27,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article813953273.txt ./dataset/data/en/dev-labels-subtask-3-spans/article813953273-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article813953273.txt ./dataset/data/en/dev-labels-subtask-3-spans/article813953273-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30/90 [00:09<00:26,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article833052347.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833052347-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 31/90 [00:09<00:26,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article813552066.txt ./dataset/data/en/dev-labels-subtask-3-spans/article813552066-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 38/90 [00:12<00:15,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article833018464.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833018464-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833018464.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833018464-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833018464.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833018464-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article833018464.txt ./dataset/data/en/dev-labels-subtask-3-spans/article833018464-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [00:21<00:06,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article817408115.txt ./dataset/data/en/dev-labels-subtask-3-spans/article817408115-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 77/90 [00:23<00:03,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/en/dev-articles-subtask-3/article832940138.txt ./dataset/data/en/dev-labels-subtask-3-spans/article832940138-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article832940138.txt ./dataset/data/en/dev-labels-subtask-3-spans/article832940138-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article832940138.txt ./dataset/data/en/dev-labels-subtask-3-spans/article832940138-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article832940138.txt ./dataset/data/en/dev-labels-subtask-3-spans/article832940138-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article832940138.txt ./dataset/data/en/dev-labels-subtask-3-spans/article832940138-labels-subtask-3.txt\n",
      "./dataset/data/en/dev-articles-subtask-3/article832940138.txt ./dataset/data/en/dev-labels-subtask-3-spans/article832940138-labels-subtask-3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:26<00:00,  3.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./dev_embeddings/article832931332_0.pt</td>\n",
       "      <td>False_Dilemma-No_Choice</td>\n",
       "      <td>Our increasing acceptance of government corrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./dev_embeddings/article832931332_1.pt</td>\n",
       "      <td>Appeal_to_Fear-Prejudice</td>\n",
       "      <td>Our increasing acceptance of government corrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./dev_embeddings/article832931332_2.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>deceit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./dev_embeddings/article832931332_3.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>erode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./dev_embeddings/article832931332_4.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>our national integrity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./dev_embeddings/article832931332_5.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>our freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./dev_embeddings/article832931332_6.pt</td>\n",
       "      <td>Repetition</td>\n",
       "      <td>What happens after no one goes to jail?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./dev_embeddings/article832931332_7.pt</td>\n",
       "      <td>Doubt</td>\n",
       "      <td>the Mueller Investigation into the alleged Tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./dev_embeddings/article832931332_8.pt</td>\n",
       "      <td>Exaggeration-Minimisation</td>\n",
       "      <td>Nowhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./dev_embeddings/article832931332_9.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>puppet show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./dev_embeddings/article832931332_10.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./dev_embeddings/article832931332_11.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>the growing list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./dev_embeddings/article832931332_12.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>the Swamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./dev_embeddings/article832931332_13.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>no there there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./dev_embeddings/article832931332_14.pt</td>\n",
       "      <td>Red_Herring</td>\n",
       "      <td>About half the adults in America knew as much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./dev_embeddings/article832931332_15.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>deadly serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./dev_embeddings/article832931332_16.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>“investigation”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./dev_embeddings/article832931332_17.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>breathless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./dev_embeddings/article832931332_18.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>“news”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./dev_embeddings/article832931332_19.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>indignantly outraged</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path                      class  \\\n",
       "0    ./dev_embeddings/article832931332_0.pt    False_Dilemma-No_Choice   \n",
       "1    ./dev_embeddings/article832931332_1.pt   Appeal_to_Fear-Prejudice   \n",
       "2    ./dev_embeddings/article832931332_2.pt            Loaded_Language   \n",
       "3    ./dev_embeddings/article832931332_3.pt            Loaded_Language   \n",
       "4    ./dev_embeddings/article832931332_4.pt                Flag_Waving   \n",
       "5    ./dev_embeddings/article832931332_5.pt                Flag_Waving   \n",
       "6    ./dev_embeddings/article832931332_6.pt                 Repetition   \n",
       "7    ./dev_embeddings/article832931332_7.pt                      Doubt   \n",
       "8    ./dev_embeddings/article832931332_8.pt  Exaggeration-Minimisation   \n",
       "9    ./dev_embeddings/article832931332_9.pt      Name_Calling-Labeling   \n",
       "10  ./dev_embeddings/article832931332_10.pt            Loaded_Language   \n",
       "11  ./dev_embeddings/article832931332_11.pt            Loaded_Language   \n",
       "12  ./dev_embeddings/article832931332_12.pt      Name_Calling-Labeling   \n",
       "13  ./dev_embeddings/article832931332_13.pt            Loaded_Language   \n",
       "14  ./dev_embeddings/article832931332_14.pt                Red_Herring   \n",
       "15  ./dev_embeddings/article832931332_15.pt      Name_Calling-Labeling   \n",
       "16  ./dev_embeddings/article832931332_16.pt      Name_Calling-Labeling   \n",
       "17  ./dev_embeddings/article832931332_17.pt            Loaded_Language   \n",
       "18  ./dev_embeddings/article832931332_18.pt      Name_Calling-Labeling   \n",
       "19  ./dev_embeddings/article832931332_19.pt            Loaded_Language   \n",
       "\n",
       "                                                 span  \n",
       "0   Our increasing acceptance of government corrup...  \n",
       "1   Our increasing acceptance of government corrup...  \n",
       "2                                              deceit  \n",
       "3                                               erode  \n",
       "4                              our national integrity  \n",
       "5                                         our freedom  \n",
       "6             What happens after no one goes to jail?  \n",
       "7   the Mueller Investigation into the alleged Tru...  \n",
       "8                                             Nowhere  \n",
       "9                                         puppet show  \n",
       "10                                                 oh  \n",
       "11                                   the growing list  \n",
       "12                                          the Swamp  \n",
       "13                                     no there there  \n",
       "14  About half the adults in America knew as much ...  \n",
       "15                                     deadly serious  \n",
       "16                                    “investigation”  \n",
       "17                                         breathless  \n",
       "18                                             “news”  \n",
       "19                               indignantly outraged  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "embedder = BertModel.from_pretrained('SpanBERT/spanbert-base-cased', add_pooling_layer=False)\n",
    "\n",
    "paths, classes, spans = [], [], []\n",
    "\n",
    "for text_path in tqdm(glob.glob('./dataset/data/en/dev-articles-subtask-3/*.txt')):\n",
    "    r = process_text(text_path, tokenizer, embedder)\n",
    "    for i in range(len(r['span_embeddings'])):\n",
    "        torch.save(r['span_embeddings'][i],\n",
    "                   './dev_embeddings/' + text_path.split('/')[-1].replace('.txt', '') + f'_{i}' + '.pt')\n",
    "        paths.append('./dev_embeddings/' + text_path.split('/')[-1].replace('.txt', '') + f'_{i}' + '.pt')\n",
    "        classes.append(r['classes'][i])\n",
    "        spans.append(r['spans'][i])\n",
    "\n",
    "df = pd.DataFrame({'path': paths, 'class': classes, 'span': spans})\n",
    "df.to_csv('dev.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 141\n",
      "7.757436752849597 10.585905826109585\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [len(s.split()) for s in df['span'].to_list()]\n",
    "bins = 100\n",
    "\n",
    "print(np.min(lens), np.max(lens))\n",
    "mu, sigma = np.mean(lens), np.std(lens)\n",
    "print(mu, sigma)\n",
    "plt.hist(lens, bins=bins)\n",
    "\n",
    "# s = np.random.lognormal(mu, sigma, 1000)\n",
    "# count, bins = np.histogram(lens, bins)\n",
    "# x = np.linspace(min(bins), max(bins), 100)\n",
    "# pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n",
    "#        / (x * sigma * np.sqrt(2 * np.pi)))\n",
    "# plt.plot(x, pdf, linewidth=2, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>./dev_embeddings/article832948083_35.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>president .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>./dev_embeddings/article813547724_1.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>Post-Brexit Britain should use its power to de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>./dev_embeddings/article832933796_3.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>happens next is “up to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>./dev_embeddings/article817408115_21.pt</td>\n",
       "      <td>Repetition</td>\n",
       "      <td>full and unfettered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>./dev_embeddings/article832940138_61.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>Here's ISIS right now,” he said. The only prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>./dev_embeddings/article832971448_30.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>Barach Obama on a boat wearing a life vest, fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>./dev_embeddings/article833039623_73.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>full-throated voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>./dev_embeddings/article822942601_10.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>near collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>./dev_embeddings/article822942601_57.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>car crash, one that will forever command an ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>./dev_embeddings/article813494037_7.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>you are Londoners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>./dev_embeddings/article813552066_76.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>In Britain, such a record could have debarred ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>./dev_embeddings/article816460196_20.pt</td>\n",
       "      <td>Doubt</td>\n",
       "      <td>That proposal has been condemned by many Brexi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>./dev_embeddings/article833036489_5.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>traitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>./dev_embeddings/article816460196_34.pt</td>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>a thundering hour-long speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>./dev_embeddings/article829267754_12.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>same deal without an exit to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>./dev_embeddings/article816460196_51.pt</td>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>and emphasized the importance of the decision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>./dev_embeddings/article829815104_7.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>hot water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>./dev_embeddings/article833053676_1.pt</td>\n",
       "      <td>Doubt</td>\n",
       "      <td>I don’t think he’s legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>./dev_embeddings/article813452859_3.pt</td>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>bitterly against it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>./dev_embeddings/article833052347_40.pt</td>\n",
       "      <td>NoClass</td>\n",
       "      <td>Trump and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path                  class  \\\n",
       "372   ./dev_embeddings/article832948083_35.pt                NoClass   \n",
       "2153   ./dev_embeddings/article813547724_1.pt            Flag_Waving   \n",
       "844    ./dev_embeddings/article832933796_3.pt                NoClass   \n",
       "2803  ./dev_embeddings/article817408115_21.pt             Repetition   \n",
       "3230  ./dev_embeddings/article832940138_61.pt                NoClass   \n",
       "1880  ./dev_embeddings/article832971448_30.pt                NoClass   \n",
       "992   ./dev_embeddings/article833039623_73.pt            Flag_Waving   \n",
       "2917  ./dev_embeddings/article822942601_10.pt        Loaded_Language   \n",
       "2964  ./dev_embeddings/article822942601_57.pt                NoClass   \n",
       "2099   ./dev_embeddings/article813494037_7.pt            Flag_Waving   \n",
       "1366  ./dev_embeddings/article813552066_76.pt            Flag_Waving   \n",
       "1980  ./dev_embeddings/article816460196_20.pt                  Doubt   \n",
       "294    ./dev_embeddings/article833036489_5.pt  Name_Calling-Labeling   \n",
       "1994  ./dev_embeddings/article816460196_34.pt  Name_Calling-Labeling   \n",
       "616   ./dev_embeddings/article829267754_12.pt                NoClass   \n",
       "2011  ./dev_embeddings/article816460196_51.pt            Flag_Waving   \n",
       "1825   ./dev_embeddings/article829815104_7.pt        Loaded_Language   \n",
       "1763   ./dev_embeddings/article833053676_1.pt                  Doubt   \n",
       "3458   ./dev_embeddings/article813452859_3.pt        Loaded_Language   \n",
       "1263  ./dev_embeddings/article833052347_40.pt                NoClass   \n",
       "\n",
       "                                                   span  \n",
       "372                                         president .  \n",
       "2153  Post-Brexit Britain should use its power to de...  \n",
       "844                              happens next is “up to  \n",
       "2803                                full and unfettered  \n",
       "3230  Here's ISIS right now,” he said. The only prob...  \n",
       "1880  Barach Obama on a boat wearing a life vest, fr...  \n",
       "992                                 full-throated voice  \n",
       "2917                                      near collapse  \n",
       "2964  car crash, one that will forever command an ho...  \n",
       "2099                                  you are Londoners  \n",
       "1366  In Britain, such a record could have debarred ...  \n",
       "1980  That proposal has been condemned by many Brexi...  \n",
       "294                                             traitor  \n",
       "1994                      a thundering hour-long speech  \n",
       "616                        same deal without an exit to  \n",
       "2011  and emphasized the importance of the decision ...  \n",
       "1825                                          hot water  \n",
       "1763                      I don’t think he’s legitimate  \n",
       "3458                                bitterly against it  \n",
       "1263                                          Trump and  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7142/2052262480.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "data = pd.read_csv('train.csv')['class'].values.reshape(-1, 1)\n",
    "onehot_encoder.fit(data)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, onehot_encoder, method='endpoint'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.labels = onehot_encoder.transform(self.data['class'].values.reshape(-1, 1))\n",
    "        self.method = method\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        file_path = self.data.iloc[idx]['path']\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        raw_embedding = torch.load(file_path)\n",
    "        if self.method == 'endpoint':\n",
    "            embedding = torch.cat((raw_embedding[:, 0, :], raw_embedding[:, -1, :]), dim=1)\n",
    "\n",
    "        # return embedding, torch.LongTensor(label)\n",
    "        return embedding, torch.tensor(label)\n",
    "    \n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'dev.csv'\n",
    "\n",
    "train_dataset = CustomDataset(train_csv_path, onehot_encoder)\n",
    "test_dataset = CustomDataset(test_csv_path, onehot_encoder)\n",
    "\n",
    "train_batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1564,  0.0624,  0.2349,  ..., -0.0516,  0.0304, -0.0379]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.], dtype=torch.float64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDeepClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1536, num_classes=20, hidden_dim=2048):\n",
    "        super(CustomDeepClassifier, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [28:33<4:25:56, 35.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500] Train Loss: 2.9136 Val Loss: 2.9723 Precision: 0.1884 Recall: 0.1114 F1 Score: 0.0860\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.10      0.05        21\n",
      "           1       0.17      0.05      0.07       130\n",
      "           2       0.00      0.29      0.01         7\n",
      "           3       0.01      0.09      0.01        35\n",
      "           4       0.02      0.05      0.03        20\n",
      "           5       0.01      0.04      0.02        26\n",
      "           6       0.36      0.08      0.13       183\n",
      "           7       0.13      0.04      0.06       119\n",
      "           8       0.28      0.20      0.23        61\n",
      "           9       0.38      0.19      0.26        94\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.64      0.07      0.12       548\n",
      "          12       0.57      0.11      0.18       285\n",
      "          13       0.98      0.22      0.36      1801\n",
      "          14       0.01      0.15      0.01        13\n",
      "          15       0.00      0.00      0.00        16\n",
      "          16       0.09      0.05      0.06       150\n",
      "          17       0.07      0.14      0.09        28\n",
      "          18       0.00      0.12      0.01         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.15      3551\n",
      "   macro avg       0.19      0.11      0.09      3551\n",
      "weighted avg       0.69      0.15      0.24      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [59:07<4:19:24, 38.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500] Train Loss: 2.9090 Val Loss: 2.9714 Precision: 0.1898 Recall: 0.1208 F1 Score: 0.1010\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.29      0.11        21\n",
      "           1       0.18      0.07      0.10       130\n",
      "           2       0.00      0.14      0.01         7\n",
      "           3       0.01      0.06      0.02        35\n",
      "           4       0.02      0.05      0.03        20\n",
      "           5       0.06      0.19      0.10        26\n",
      "           6       0.44      0.09      0.15       183\n",
      "           7       0.17      0.06      0.09       119\n",
      "           8       0.19      0.16      0.18        61\n",
      "           9       0.42      0.19      0.26        94\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.53      0.12      0.19       548\n",
      "          12       0.48      0.08      0.13       285\n",
      "          13       0.99      0.29      0.45      1801\n",
      "          14       0.00      0.15      0.01        13\n",
      "          15       0.01      0.12      0.01        16\n",
      "          16       0.12      0.05      0.07       150\n",
      "          17       0.08      0.18      0.11        28\n",
      "          18       0.00      0.12      0.00         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.20      3551\n",
      "   macro avg       0.19      0.12      0.10      3551\n",
      "weighted avg       0.68      0.20      0.30      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [1:30:22<3:49:49, 39.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/500] Train Loss: 2.9072 Val Loss: 2.9701 Precision: 0.2067 Recall: 0.1289 F1 Score: 0.1088\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.14      0.04        21\n",
      "           1       0.23      0.09      0.13       130\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.03      0.00        35\n",
      "           4       0.03      0.10      0.05        20\n",
      "           5       0.09      0.31      0.14        26\n",
      "           6       0.41      0.10      0.16       183\n",
      "           7       0.27      0.11      0.15       119\n",
      "           8       0.11      0.16      0.13        61\n",
      "           9       0.41      0.17      0.24        94\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.66      0.17      0.27       548\n",
      "          12       0.61      0.08      0.14       285\n",
      "          13       0.99      0.30      0.47      1801\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.01      0.12      0.01        16\n",
      "          16       0.24      0.10      0.14       150\n",
      "          17       0.06      0.21      0.09        28\n",
      "          18       0.01      0.12      0.01         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.22      3551\n",
      "   macro avg       0.21      0.13      0.11      3551\n",
      "weighted avg       0.72      0.22      0.32      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [2:00:41<3:08:40, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/500] Train Loss: 2.9038 Val Loss: 2.9692 Precision: 0.1990 Recall: 0.1467 F1 Score: 0.1216\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.19      0.06        21\n",
      "           1       0.16      0.07      0.10       130\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.09      0.02        35\n",
      "           4       0.03      0.15      0.05        20\n",
      "           5       0.04      0.23      0.07        26\n",
      "           6       0.49      0.21      0.29       183\n",
      "           7       0.18      0.10      0.13       119\n",
      "           8       0.12      0.23      0.16        61\n",
      "           9       0.53      0.21      0.30        94\n",
      "          10       0.00      0.25      0.01         4\n",
      "          11       0.65      0.21      0.32       548\n",
      "          12       0.62      0.14      0.23       285\n",
      "          13       0.99      0.37      0.54      1801\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.00      0.00      0.00        16\n",
      "          16       0.06      0.03      0.04       150\n",
      "          17       0.06      0.32      0.10        28\n",
      "          18       0.00      0.12      0.01         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.27      3551\n",
      "   macro avg       0.20      0.15      0.12      3551\n",
      "weighted avg       0.71      0.27      0.38      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 250/500 [2:31:46<2:38:10, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/500] Train Loss: 2.9033 Val Loss: 2.9688 Precision: 0.1998 Recall: 0.1629 F1 Score: 0.1328\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.10      0.03        21\n",
      "           1       0.19      0.11      0.14       130\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.06      0.02        35\n",
      "           4       0.03      0.15      0.05        20\n",
      "           5       0.02      0.12      0.03        26\n",
      "           6       0.48      0.20      0.28       183\n",
      "           7       0.19      0.12      0.14       119\n",
      "           8       0.10      0.20      0.14        61\n",
      "           9       0.38      0.26      0.30        94\n",
      "          10       0.01      0.50      0.01         4\n",
      "          11       0.68      0.23      0.34       548\n",
      "          12       0.67      0.27      0.38       285\n",
      "          13       0.99      0.42      0.59      1801\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.01      0.06      0.01        16\n",
      "          16       0.20      0.09      0.13       150\n",
      "          17       0.03      0.14      0.04        28\n",
      "          18       0.01      0.25      0.02         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.31      3551\n",
      "   macro avg       0.20      0.16      0.13      3551\n",
      "weighted avg       0.72      0.31      0.43      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [3:03:01<2:13:20, 40.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/500] Train Loss: 2.9033 Val Loss: 2.9672 Precision: 0.2078 Recall: 0.2019 F1 Score: 0.1461\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.14      0.04        21\n",
      "           1       0.30      0.13      0.18       130\n",
      "           2       0.00      0.14      0.01         7\n",
      "           3       0.03      0.11      0.04        35\n",
      "           4       0.03      0.15      0.06        20\n",
      "           5       0.03      0.19      0.06        26\n",
      "           6       0.50      0.23      0.32       183\n",
      "           7       0.25      0.18      0.21       119\n",
      "           8       0.11      0.28      0.16        61\n",
      "           9       0.43      0.24      0.31        94\n",
      "          10       0.01      0.75      0.03         4\n",
      "          11       0.70      0.23      0.35       548\n",
      "          12       0.61      0.24      0.34       285\n",
      "          13       0.98      0.50      0.66      1801\n",
      "          14       0.01      0.15      0.02        13\n",
      "          15       0.01      0.06      0.01        16\n",
      "          16       0.09      0.05      0.07       150\n",
      "          17       0.02      0.11      0.04        28\n",
      "          18       0.01      0.12      0.01         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.35      3551\n",
      "   macro avg       0.21      0.20      0.15      3551\n",
      "weighted avg       0.72      0.35      0.46      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 350/500 [3:33:35<1:34:54, 37.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/500] Train Loss: 2.9020 Val Loss: 2.9672 Precision: 0.2109 Recall: 0.1663 F1 Score: 0.1479\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.14      0.05        21\n",
      "           1       0.19      0.10      0.13       130\n",
      "           2       0.01      0.14      0.01         7\n",
      "           3       0.01      0.03      0.01        35\n",
      "           4       0.04      0.15      0.06        20\n",
      "           5       0.04      0.19      0.07        26\n",
      "           6       0.51      0.21      0.30       183\n",
      "           7       0.18      0.11      0.14       119\n",
      "           8       0.13      0.20      0.16        61\n",
      "           9       0.48      0.29      0.36        94\n",
      "          10       0.01      0.25      0.01         4\n",
      "          11       0.66      0.28      0.39       548\n",
      "          12       0.70      0.24      0.35       285\n",
      "          13       0.98      0.51      0.67      1801\n",
      "          14       0.01      0.08      0.01        13\n",
      "          15       0.00      0.06      0.01        16\n",
      "          16       0.19      0.10      0.13       150\n",
      "          17       0.06      0.25      0.09        28\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.36      3551\n",
      "   macro avg       0.21      0.17      0.15      3551\n",
      "weighted avg       0.72      0.36      0.47      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [4:04:02<1:03:27, 38.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/500] Train Loss: 2.9026 Val Loss: 2.9682 Precision: 0.2124 Recall: 0.1798 F1 Score: 0.1527\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.10      0.03        21\n",
      "           1       0.23      0.10      0.14       130\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.01      0.06      0.02        35\n",
      "           4       0.03      0.10      0.05        20\n",
      "           5       0.03      0.19      0.05        26\n",
      "           6       0.54      0.21      0.31       183\n",
      "           7       0.19      0.11      0.14       119\n",
      "           8       0.22      0.25      0.23        61\n",
      "           9       0.35      0.28      0.31        94\n",
      "          10       0.01      0.50      0.02         4\n",
      "          11       0.74      0.26      0.38       548\n",
      "          12       0.63      0.28      0.38       285\n",
      "          13       0.99      0.51      0.68      1801\n",
      "          14       0.01      0.15      0.02        13\n",
      "          15       0.00      0.06      0.01        16\n",
      "          16       0.15      0.12      0.13       150\n",
      "          17       0.11      0.32      0.16        28\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.36      3551\n",
      "   macro avg       0.21      0.18      0.15      3551\n",
      "weighted avg       0.73      0.36      0.48      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 450/500 [4:34:31<31:46, 38.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [450/500] Train Loss: 2.9021 Val Loss: 2.9686 Precision: 0.2038 Recall: 0.2304 F1 Score: 0.1555\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.14      0.05        21\n",
      "           1       0.29      0.15      0.19       130\n",
      "           2       0.01      0.29      0.02         7\n",
      "           3       0.03      0.11      0.04        35\n",
      "           4       0.06      0.25      0.10        20\n",
      "           5       0.01      0.08      0.02        26\n",
      "           6       0.43      0.23      0.30       183\n",
      "           7       0.19      0.12      0.15       119\n",
      "           8       0.14      0.28      0.19        61\n",
      "           9       0.42      0.22      0.29        94\n",
      "          10       0.03      1.00      0.05         4\n",
      "          11       0.64      0.33      0.44       548\n",
      "          12       0.66      0.27      0.38       285\n",
      "          13       0.97      0.57      0.72      1801\n",
      "          14       0.01      0.08      0.01        13\n",
      "          15       0.02      0.12      0.03        16\n",
      "          16       0.11      0.07      0.08       150\n",
      "          17       0.02      0.18      0.04        28\n",
      "          18       0.01      0.12      0.01         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.41      3551\n",
      "   macro avg       0.20      0.23      0.16      3551\n",
      "weighted avg       0.70      0.41      0.51      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [5:06:13<00:00, 36.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/500] Train Loss: 2.9012 Val Loss: 2.9674 Precision: 0.2100 Recall: 0.2139 F1 Score: 0.1577\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.19      0.06        21\n",
      "           1       0.32      0.11      0.16       130\n",
      "           2       0.01      0.14      0.01         7\n",
      "           3       0.01      0.11      0.03        35\n",
      "           4       0.03      0.10      0.04        20\n",
      "           5       0.04      0.15      0.06        26\n",
      "           6       0.40      0.21      0.27       183\n",
      "           7       0.19      0.13      0.15       119\n",
      "           8       0.13      0.21      0.16        61\n",
      "           9       0.41      0.23      0.30        94\n",
      "          10       0.02      0.75      0.04         4\n",
      "          11       0.67      0.31      0.43       548\n",
      "          12       0.68      0.29      0.40       285\n",
      "          13       0.97      0.60      0.74      1801\n",
      "          14       0.02      0.23      0.03        13\n",
      "          15       0.01      0.06      0.01        16\n",
      "          16       0.19      0.11      0.14       150\n",
      "          17       0.07      0.21      0.10        28\n",
      "          18       0.01      0.12      0.01         8\n",
      "          19       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.42      3551\n",
      "   macro avg       0.21      0.21      0.16      3551\n",
      "weighted avg       0.71      0.42      0.52      3551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_validate_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.1, stepslr=10, gamma=0.9):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, stepslr, gamma=gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            # print(outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if (epoch + 1) % stepslr == 0:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = criterion(outputs.squeeze(), labels)\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "                    predicted = torch.argmax(outputs.squeeze(1), 1)\n",
    "                    labels = torch.argmax(labels, 1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "                f\"Train Loss: {train_loss:.4f} \"\n",
    "                f\"Val Loss: {val_loss:.4f} \"\n",
    "                f\"Precision: {precision:.4f} \"\n",
    "                f\"Recall: {recall:.4f} \"\n",
    "                f\"F1 Score: {f1:.4f}\\n\")\n",
    "            print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return train_losses, val_losses, precision_scores, recall_scores, f1_scores\n",
    "\n",
    "\n",
    "model = CustomDeepClassifier(input_dim=train_dataset[0][0].shape[1], num_classes=20)\n",
    "train_losses, val_losses, precision_scores, recall_scores, f1_scores = train_validate_model(model,\n",
    "                                                                                            train_data_loader,\n",
    "                                                                                            test_data_loader,\n",
    "                                                                                            num_epochs=500,\n",
    "                                                                                            learning_rate=3e-4,\n",
    "                                                                                            stepslr=50,\n",
    "                                                                                            gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with top 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "data = pd.read_csv('train_top5.csv')['class'].values.reshape(-1, 1)\n",
    "onehot_encoder.fit(data)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, onehot_encoder, method='endpoint'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.labels = onehot_encoder.transform(self.data['class'].values.reshape(-1, 1))\n",
    "        self.method = method\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        file_path = self.data.iloc[idx]['path']\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        raw_embedding = torch.load(file_path)\n",
    "        if self.method == 'endpoint':\n",
    "            embedding = torch.cat((raw_embedding[:, 0, :], raw_embedding[:, -1, :]), dim=1)\n",
    "\n",
    "        # return embedding, torch.LongTensor(label)\n",
    "        return embedding, torch.tensor(label)\n",
    "    \n",
    "train_csv_path = 'train_top5.csv'\n",
    "test_csv_path = 'dev_top5.csv'\n",
    "\n",
    "train_dataset = CustomDataset(train_csv_path, onehot_encoder)\n",
    "test_dataset = CustomDataset(test_csv_path, onehot_encoder)\n",
    "\n",
    "train_batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1564,  0.0624,  0.2349,  ..., -0.0516,  0.0304, -0.0379]]),\n",
       " tensor([0., 0., 1., 0., 0., 0.], dtype=torch.float64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDeepClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1536, num_classes=20, hidden_dim=2048):\n",
    "        super(CustomDeepClassifier, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(1))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 30/300 [16:30<2:33:15, 34.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300] Train Loss: 1.7608 Val Loss: 1.7755 Precision: 0.4430 Recall: 0.3735 F1 Score: 0.3966\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.39      0.43       183\n",
      "           1       0.20      0.14      0.17       119\n",
      "           2       0.54      0.36      0.43       548\n",
      "           3       0.52      0.28      0.36       285\n",
      "           4       0.74      0.89      0.81      1801\n",
      "           5       0.18      0.17      0.18       150\n",
      "\n",
      "    accuracy                           0.65      3086\n",
      "   macro avg       0.44      0.37      0.40      3086\n",
      "weighted avg       0.62      0.65      0.62      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 60/300 [32:10<2:09:13, 32.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/300] Train Loss: 1.7600 Val Loss: 1.7770 Precision: 0.3903 Recall: 0.4088 F1 Score: 0.3941\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.48      0.44       183\n",
      "           1       0.14      0.23      0.18       119\n",
      "           2       0.51      0.42      0.46       548\n",
      "           3       0.33      0.33      0.33       285\n",
      "           4       0.84      0.78      0.80      1801\n",
      "           5       0.12      0.22      0.16       150\n",
      "\n",
      "    accuracy                           0.61      3086\n",
      "   macro avg       0.39      0.41      0.39      3086\n",
      "weighted avg       0.64      0.61      0.62      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 90/300 [47:43<1:53:03, 32.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/300] Train Loss: 1.7589 Val Loss: 1.7752 Precision: 0.4044 Recall: 0.3985 F1 Score: 0.3853\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.45      0.40       183\n",
      "           1       0.19      0.17      0.18       119\n",
      "           2       0.55      0.34      0.42       548\n",
      "           3       0.40      0.28      0.33       285\n",
      "           4       0.80      0.80      0.80      1801\n",
      "           5       0.13      0.35      0.18       150\n",
      "\n",
      "    accuracy                           0.60      3086\n",
      "   macro avg       0.40      0.40      0.39      3086\n",
      "weighted avg       0.64      0.60      0.61      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 120/300 [1:03:18<1:37:08, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/300] Train Loss: 1.7577 Val Loss: 1.7743 Precision: 0.4208 Recall: 0.4237 F1 Score: 0.3975\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.44      0.44       183\n",
      "           1       0.15      0.22      0.17       119\n",
      "           2       0.50      0.43      0.46       548\n",
      "           3       0.42      0.29      0.34       285\n",
      "           4       0.91      0.70      0.79      1801\n",
      "           5       0.11      0.47      0.17       150\n",
      "\n",
      "    accuracy                           0.57      3086\n",
      "   macro avg       0.42      0.42      0.40      3086\n",
      "weighted avg       0.69      0.57      0.62      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 150/300 [1:18:52<1:21:02, 32.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/300] Train Loss: 1.7577 Val Loss: 1.7749 Precision: 0.4075 Recall: 0.3940 F1 Score: 0.3856\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.33      0.36       183\n",
      "           1       0.12      0.22      0.15       119\n",
      "           2       0.52      0.49      0.51       548\n",
      "           3       0.43      0.30      0.35       285\n",
      "           4       0.89      0.74      0.80      1801\n",
      "           5       0.09      0.29      0.13       150\n",
      "\n",
      "    accuracy                           0.59      3086\n",
      "   macro avg       0.41      0.39      0.39      3086\n",
      "weighted avg       0.68      0.59      0.63      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 180/300 [1:34:27<1:04:53, 32.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/300] Train Loss: 1.7572 Val Loss: 1.7767 Precision: 0.4004 Recall: 0.4246 F1 Score: 0.3981\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.39      0.39       183\n",
      "           1       0.13      0.25      0.17       119\n",
      "           2       0.51      0.50      0.50       548\n",
      "           3       0.34      0.38      0.36       285\n",
      "           4       0.92      0.70      0.80      1801\n",
      "           5       0.11      0.32      0.16       150\n",
      "\n",
      "    accuracy                           0.58      3086\n",
      "   macro avg       0.40      0.42      0.40      3086\n",
      "weighted avg       0.69      0.58      0.62      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 210/300 [1:50:05<48:41, 32.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/300] Train Loss: 1.7568 Val Loss: 1.7754 Precision: 0.4078 Recall: 0.4291 F1 Score: 0.3951\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.48      0.43       183\n",
      "           1       0.13      0.17      0.15       119\n",
      "           2       0.58      0.48      0.53       548\n",
      "           3       0.32      0.38      0.35       285\n",
      "           4       0.92      0.65      0.76      1801\n",
      "           5       0.10      0.43      0.16       150\n",
      "\n",
      "    accuracy                           0.55      3086\n",
      "   macro avg       0.41      0.43      0.40      3086\n",
      "weighted avg       0.70      0.55      0.61      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 240/300 [2:05:40<32:22, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/300] Train Loss: 1.7585 Val Loss: 1.7742 Precision: 0.4155 Recall: 0.3772 F1 Score: 0.3692\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.25      0.32       183\n",
      "           1       0.15      0.19      0.17       119\n",
      "           2       0.59      0.37      0.46       548\n",
      "           3       0.37      0.28      0.32       285\n",
      "           4       0.87      0.76      0.81      1801\n",
      "           5       0.09      0.41      0.14       150\n",
      "\n",
      "    accuracy                           0.58      3086\n",
      "   macro avg       0.42      0.38      0.37      3086\n",
      "weighted avg       0.68      0.58      0.62      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 270/300 [2:22:08<17:55, 35.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/300] Train Loss: 1.7576 Val Loss: 1.7750 Precision: 0.4116 Recall: 0.4214 F1 Score: 0.3816\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.52      0.44       183\n",
      "           1       0.15      0.18      0.16       119\n",
      "           2       0.57      0.28      0.37       548\n",
      "           3       0.38      0.29      0.33       285\n",
      "           4       0.89      0.74      0.81      1801\n",
      "           5       0.11      0.52      0.18       150\n",
      "\n",
      "    accuracy                           0.57      3086\n",
      "   macro avg       0.41      0.42      0.38      3086\n",
      "weighted avg       0.69      0.57      0.61      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [2:38:26<00:00, 31.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300] Train Loss: 1.7577 Val Loss: 1.7747 Precision: 0.4211 Recall: 0.4282 F1 Score: 0.3888\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.54      0.44       183\n",
      "           1       0.17      0.27      0.21       119\n",
      "           2       0.56      0.32      0.41       548\n",
      "           3       0.41      0.35      0.38       285\n",
      "           4       0.93      0.64      0.76      1801\n",
      "           5       0.08      0.45      0.14       150\n",
      "\n",
      "    accuracy                           0.53      3086\n",
      "   macro avg       0.42      0.43      0.39      3086\n",
      "weighted avg       0.71      0.53      0.59      3086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_validate_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.1, stepslr=10, gamma=0.9):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, stepslr, gamma=gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if (epoch + 1) % stepslr == 0:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = criterion(outputs.squeeze(), labels)\n",
    "                    running_val_loss += loss.item()\n",
    "\n",
    "                    predicted = torch.argmax(outputs.squeeze(1), 1)\n",
    "                    labels = torch.argmax(labels, 1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "                f\"Train Loss: {train_loss:.4f} \"\n",
    "                f\"Val Loss: {val_loss:.4f} \"\n",
    "                f\"Precision: {precision:.4f} \"\n",
    "                f\"Recall: {recall:.4f} \"\n",
    "                f\"F1 Score: {f1:.4f}\\n\")\n",
    "            print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return train_losses, val_losses, precision_scores, recall_scores, f1_scores\n",
    "\n",
    "\n",
    "model = CustomDeepClassifier(input_dim=train_dataset[0][0].shape[1], num_classes=6)\n",
    "train_losses, val_losses, precision_scores, recall_scores, f1_scores = train_validate_model(model,\n",
    "                                                                                            train_data_loader,\n",
    "                                                                                            test_data_loader,\n",
    "                                                                                            num_epochs=300,\n",
    "                                                                                            learning_rate=3e-2,\n",
    "                                                                                            stepslr=30,\n",
    "                                                                                            gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
