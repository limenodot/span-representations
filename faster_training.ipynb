{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8230/3547758232.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pprint import pprint\n",
    "from belt_nlp.splitting import transform_single_text\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_gui\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text_path, tokenizer, embedder):\n",
    "    with open(text_path, 'r') as text_file:\n",
    "        text = text_file.read()\n",
    "\n",
    "    labels_path = text_path.replace('.txt', '-labels-subtask-3.txt').replace('-articles-subtask-3', '-labels-subtask-3-spans')\n",
    "    with open(labels_path, 'r') as labels_file:\n",
    "        labels = labels_file.read()\n",
    "        labels = [l.split('\\t') for l in labels.split('\\n')][:-1]\n",
    "    \n",
    "    res = dict()\n",
    "\n",
    "    tokens = tokenizer(text, add_special_tokens=False, truncation=False, return_tensors='pt')\n",
    "    res['tokens'] = tokens\n",
    "    res['num_tokens'] = len(tokens['input_ids'][0])\n",
    "    input_ids, attention_mask = transform_single_text(\n",
    "        text,\n",
    "        tokenizer,\n",
    "        chunk_size=510,\n",
    "        stride=300,\n",
    "        minimal_chunk_length=20,\n",
    "        maximal_text_length=None\n",
    "    )\n",
    "    res['chunks'] = input_ids\n",
    "    res['attn_mask'] = attention_mask\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    embedder.to(device)\n",
    "    embeddingss = []\n",
    "    for i in range(len(res['chunks'])):\n",
    "        with torch.no_grad():\n",
    "            output = embedder(res['chunks'][i].unsqueeze(0).to(device), res['attn_mask'][i].unsqueeze(0).to(device))\n",
    "        _start = 0 if i == 0 else 210\n",
    "        _end = list(res['attn_mask'][i]).index(0) if 0 in res['attn_mask'][i] else 0\n",
    "        last_hidden_state = output.last_hidden_state[:, 1+_start:-1+_end, :].detach().cpu()\n",
    "        embeddingsss.append(last_hidden_state)\n",
    "    embeddingss = torch.cat(embeddingss, dim=1)\n",
    "\n",
    "    # SPANS\n",
    "    spans = [text[int(l[2]):int(l[3])] for l in labels]\n",
    "    res['spans'] = spans\n",
    "\n",
    "    # NONE CLASS\n",
    "    space_ids = [i for i in range(len(text)) if ' ' == text[i]]\n",
    "    # space_ids = [i for i in range(len(text)) if re.fullmatch(r'\\s', text[i])]\n",
    "    span_edges = [(int(l[2]), int(l[3])) for l in labels]\n",
    "    def overlap(s, e, edgs):\n",
    "        return any([not (e < edgs[i][0] or s > edgs[i][1]) for i in range(len(edgs))])\n",
    "    \n",
    "    none_classes, none_spans = [], []\n",
    "    for i in range(len(spans)):\n",
    "        _len = len(spans[i].split())\n",
    "        start = space_ids.index(np.random.choice(space_ids))\n",
    "        end = start + _len\n",
    "        counter = 1\n",
    "        while overlap(start, end, span_edges) and counter < 200:\n",
    "            start = space_ids.index(np.random.choice(space_ids))\n",
    "            end = start + _len\n",
    "            counter += 1\n",
    "        none_spans.append(' '.join(text.split()[start:end + 1]))\n",
    "        none_classes.append('NoClass')\n",
    "\n",
    "    res['spans'].extend(none_spans)\n",
    "\n",
    "    # SPAN EMDEDDINGS\n",
    "    span_tokens = [\n",
    "        tokenizer(span, add_special_tokens=False, truncation=False)\n",
    "        for span in res['spans']\n",
    "    ]\n",
    "    span_embeddingss = []\n",
    "    res['classes'] = []\n",
    "    for j in range(len(span_tokens)):\n",
    "        try:\n",
    "            for i in range(len(tokens['input_ids'][0])):\n",
    "                if (tokens['input_ids'][0][i:i + len(span_tokens[j]['input_ids'])].numpy() == span_tokens[j]['input_ids']).all():\n",
    "                    span_embeddingss.append(embeddingss[:, i:i + len(span_tokens[j]['input_ids']), :])\n",
    "                    try:\n",
    "                        res['classes'].append(labels[j][1])\n",
    "                    except IndexError as e:\n",
    "                        pass\n",
    "                    break\n",
    "        except ValueError as e:\n",
    "            print(text_path, labels_path)\n",
    "    res['span_embeddingss'] = span_embeddingss\n",
    "\n",
    "    res['classes'].extend(none_classes)\n",
    "\n",
    "    text = text.lower()\n",
    "    for l in labels:\n",
    "        text = text.replace(text[int(l[2]):int(l[3])], text[int(l[2]):int(l[3])].upper())\n",
    "    res['text'] = text\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuznecovaa/test/.venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "data = pd.read_csv('train.csv')['class'].values.reshape(-1, 1)\n",
    "onehot_encoder.fit(data)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "embedder = BertModel.from_pretrained('SpanBERT/spanbert-base-cased', add_pooling_layer=False)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, onehot_encoder, tokenizer, embedder, padding_size=256, method='endpoint'):\n",
    "        self.files = files\n",
    "        self.ohe = onehot_encoder\n",
    "        self.method = method\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "        self.padding_size = padding_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        r = process_text(self.files[idx], self.tokenizer, self.embedder)\n",
    "        label = r['classes']\n",
    "\n",
    "        raw_embeddings = r['span_embeddings']\n",
    "        if self.method == 'endpoint':\n",
    "            embeddings = [torch.cat((re[:, 0, :], re[:, -1, :]), dim=1) for re in raw_embeddings]\n",
    "        elif self.method == 'diff-sum':\n",
    "            embeddings = [torch.cat(\n",
    "                (\n",
    "                    re[:, 0, :] + re[:, -1, :],\n",
    "                    re[:, 0, :] - re[:, -1, :]\n",
    "                ),\n",
    "                dim=1\n",
    "            ) for re in raw_embeddings]\n",
    "        elif self.method == 'coherent':\n",
    "            embeddings = [torch.cat(\n",
    "                (\n",
    "                    re[:, 0, :360],\n",
    "                    re[:, -1, 360:720],\n",
    "                    torch.dot(\n",
    "                        re[:, 0, :].squeeze()[720:],\n",
    "                        re[:, -1, :].squeeze()[720:]\n",
    "                    ).unsqueeze(0).unsqueeze(0)\n",
    "                ),\n",
    "                dim=1\n",
    "            ) for re in raw_embeddings]\n",
    "        elif self.method == 'maxpool':\n",
    "            embeddings = [torch.max(re, dim=1)[0] for re in raw_embeddings]\n",
    "        elif self.method == 'avgpool':\n",
    "            embeddings = [torch.mean(re, dim=1) for re in raw_embeddings]\n",
    "        else:\n",
    "            embeddings = raw_embeddings\n",
    "            padding = [torch.zeros(1, self.padding_size - e.shape[1], e.shape[-1]) for e in embeddings]\n",
    "            embeddings = [torch.cat((e, p), dim=1) for e, p in zip(embeddings, padding)]\n",
    "\n",
    "        return embeddings, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'dev.csv'\n",
    "\n",
    "train_dataset = CustomDataset(train_csv_path, onehot_encoder)\n",
    "test_dataset = CustomDataset(test_csv_path, onehot_encoder)\n",
    "\n",
    "train_batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
